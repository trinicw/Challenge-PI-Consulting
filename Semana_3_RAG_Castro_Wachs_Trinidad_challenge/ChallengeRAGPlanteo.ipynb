{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0dac1e6e-28c7-49d6-b0b7-503543ae7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "# Cargar las API keys necesarias\n",
    "# Hacer las conexiones necesarias\n",
    "#!pip install -U langchain-community\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import chromadb # Biblioteca para gestionar bases de datos de vectores y para trabajar con embeddings.\n",
    "import cohere   # Cliente para interactuar con los modelos LLM de Cohere.\n",
    "from dotenv import load_dotenv\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Se utiliza para dividir el texto en fragmentos más pequeños (chunks) respetando un tamaño máximo y un solapamiento definido.\n",
    "from PyPDF2 import PdfReader  # Se emplea para leer y extraer el texto de archivos PDF.\n",
    "from langchain.embeddings import CohereEmbeddings  # Clase para trabajar con embeddings generados por Cohere.\n",
    "from langchain.vectorstores import Chroma # Integra almacenamiento vectorial con embeddings para búsquedas eficientes.\n",
    "from langchain.document_loaders import TextLoader  # Carga documentos de texto para su procesamiento posterior.\n",
    "\n",
    "\n",
    "#!pip install langchain PyPDF2\n",
    "#!pip install chromadb langchain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "#print(api_key)  # Verify the key is loaded\n",
    "\n",
    "\n",
    "# Establezco la conexion a cohere para el embedding y hago la primer consulta con el modelo de embeddings a modo de prueba.\n",
    "# Uso la api version v2\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "response = co.embed(\n",
    "    texts=[\"hola\"],\n",
    "    model=\"embed-multilingual-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    "    embedding_types=[\"float\"],\n",
    ")\n",
    "\n",
    "\n",
    "#print(response)\n",
    "#print(response.embeddings.float_[0])\n",
    "#print(len(response.embeddings.float_[0]))\n",
    "\n",
    "\n",
    "# Establezco la conexion a cohere y realizo una primer consulta a algun modelo del LLM a modo de prueba.\n",
    "# Utilizo la api version v2\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"¿Qué país ganó el mundial 2022?\"}],\n",
    ")\n",
    "\n",
    "#print(response.message.content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5fd50a62-445b-4f7f-a222-12be8c64defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los documentos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "24fe6c58-714c-42cb-9250-b36b6f0c5e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historia: Sol y Luna\n",
      "Cantidad de chunks generados: 13\n",
      "Chunk 1:\n",
      "Sol y Luna \n",
      "Sol y Luna eran dos pequeños gatitos que habían nacido en la misma camada, pero sus \n",
      "personalidades no podían ser más diferentes. Sol, de un brillante color anaranjado, era \n",
      "aventurero y curioso; siempre buscando nuevas experiencias y explorando cada rincón de\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "aventurero y curioso; siempre buscando nuevas experiencias y explorando cada rincón de \n",
      "su hogar. Luna, en cambio, era de un suave pelaje gris y tenía un temperamento más \n",
      "sereno y observador. Pasaba horas contemplando el mundo desde la ventana, como si en\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "sereno y observador. Pasaba horas contemplando el mundo desde la ventana, como si en \n",
      "cada sombra descubriera un misterio oculto.  \n",
      "Una tarde, mientras Sol correteaba por el jardín persiguiendo mariposas, Luna permanecía\n",
      "--------------------------------------------------\n",
      "Historia: La tortuga Tica\n",
      "Cantidad de chunks generados: 9\n",
      "Chunk 1:\n",
      "La tortuga Tica \n",
      "Una tortuga llamada Tica vivía en un tranquilo estanque rodeado de árboles frondosos. A \n",
      "diferencia de sus compañeras, Tica soñaba con explorar más allá del agua tranquila y la\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "diferencia de sus compañeras, Tica soñaba con explorar más allá del agua tranquila y la \n",
      "suave hierba. Un día, decidió emprender una aventura, dejando atrás la comodidad  de su hogar. Se adentró en el bosque, donde todo era nuevo: el susurro de las hojas, los aromas\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "desconocidos y el crujir de las ramas bajo sus patas lentas pero firmes.  \n",
      "Durante su viaje, Tica encontró un riachuelo de corriente rápida. Al principio, la idea de \n",
      "cruzarlo la asustó, pero recordó que cada desafío era una oportunidad. Con paciencia y\n",
      "--------------------------------------------------\n",
      "Historia: El Duende\n",
      "Cantidad de chunks generados: 9\n",
      "Chunk 1:\n",
      "El Duende  \n",
      "Había un pequeño duende llamado Puck, conocido por su espíritu travieso y su amor por \n",
      "las bromas. Vivía en lo profundo del bosque, donde las criaturas del lugar sabían que, si \n",
      "algo extraño sucedía, era obra de él. Puck disfrutaba de hacer desaparecer objet os,\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "algo extraño sucedía, era obra de él. Puck disfrutaba de hacer desaparecer objet os, \n",
      "cambiar las señales de los senderos y provocar pequeñas confusiones entre los animales. \n",
      "Sin embargo, su diversión nunca era malintencionada; simplemente, amaba ver las \n",
      "reacciones sorprendidas de los demás.\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "reacciones sorprendidas de los demás.  \n",
      "Un día, decidió que quería jugarle una broma a la anciana hada que vivía cerca del arroyo. \n",
      "Ella, conocida por su sabiduría, siempre estaba en silencio, tejiendo sueños y\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader # Se utiliza para leer y extraer texto de archivos PDF.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Herramienta para dividir texto en fragmentos de tamaño manejable.\n",
    "\n",
    "# Cargar el contenido del PDF\n",
    "def cargar_pdf(ruta_pdf):\n",
    "    \"\"\"\n",
    "    Carga el contenido de un archivo PDF y extrae su texto.\n",
    "    \n",
    "    Args:\n",
    "        ruta_pdf (str): Ruta relativa del archivo PDF.\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto completo extraído del PDF.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(ruta_pdf)\n",
    "    texto = \"\"\n",
    "    for page in reader.pages:              # Itera sobre todas las páginas del PDF.\n",
    "        texto += page.extract_text()       # Extrae el texto de cada página y lo agrega al texto completo.\n",
    "    return texto\n",
    "\n",
    "# Dividir el texto por historias\n",
    "def dividir_por_historias(texto):\n",
    "    \"\"\"\n",
    "    Divide el texto en historias usando títulos identificados.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto completo del PDF.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con el título de la historia como clave y su texto como valor.\n",
    "    \"\"\"\n",
    "    separadores = [\"Sol y Luna\", \"La tortuga Tica\", \"El Duende\"]\n",
    "    historias = {}\n",
    "    for i, titulo in enumerate(separadores):\n",
    "        inicio = texto.find(titulo)  # Encuentra la posición del título actual\n",
    "        fin = texto.find(separadores[i + 1]) if i + 1 < len(separadores) else len(texto) # Encuentra la posición del siguiente título o el final del texto.\n",
    "        historias[titulo] = texto[inicio:fin].strip()  # Extrae el texto correspondiente a la historia y elimina espacios adicionales.\n",
    "    return historias # Retorna un diccionario con las historias separadas.\n",
    "\n",
    "\n",
    "# Aplicar chunking a cada historia\n",
    "def aplicar_chunking_a_historias(historias, chunk_size=300, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Aplica chunking a cada historia individualmente con chunks pequeños.\n",
    "    \n",
    "    Args:\n",
    "        historias (dict): Diccionario con historias separadas.\n",
    "        chunk_size (int): Longitud máxima de cada chunk.\n",
    "        chunk_overlap (int): Superposición entre chunks.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con chunks de cada historia.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap) # Configura el divisor de texto con tamaño máximo y superposición\n",
    "    chunks_por_historia = {}\n",
    "    for titulo, texto in historias.items():\n",
    "        chunks = splitter.split_text(texto)  # Divide el texto de la historia en chunks.\n",
    "        chunks_por_historia[titulo] = chunks # Asocia los chunks al título de la historia.\n",
    "    return chunks_por_historia  # Retorna un diccionario con los chunks por historia.\n",
    "\n",
    "# Ruta del archivo PDF\n",
    "ruta_pdf = \"Historias.pdf\"\n",
    "\n",
    "# Cargar el PDF\n",
    "texto_pdf = cargar_pdf(ruta_pdf)\n",
    "\n",
    "# Dividir el texto por historias\n",
    "historias = dividir_por_historias(texto_pdf)\n",
    "\n",
    "# Aplicar chunking a cada historia (ajustado a más chunks) individualmente.\n",
    "chunks_por_historia = aplicar_chunking_a_historias(historias)  \n",
    "\n",
    "# Mostrar los resultados\n",
    "for titulo, chunks in chunks_por_historia.items():\n",
    "    print(f\"Historia: {titulo}\")\n",
    "    print(f\"Cantidad de chunks generados: {len(chunks)}\")\n",
    "    for i, chunk in enumerate(chunks[:3]):  # Mostrar los primeros 3 chunks por historia como ejemplo\n",
    "        print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*50}\")\n",
    "\n",
    "\n",
    "\n",
    "# Nota: Optamos por la técnica de chunking con un tamaño de chunk de 300 caracteres y una superposición de 100 porque:\n",
    "# - El tamaño de chunk (300) es lo suficientemente pequeño para asegurar que cada fragmento sea procesable y manejable por modelos de lenguaje o sistemas de búsquedasda.\n",
    "# - La superposición (100) garantiza que las ideas no se corten abruptamente entre chunks, manteniendo el contexto entre fragmentos consecutivos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "adc7d97e-9df2-4347-b748-177118d177be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexion a la base de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "296a1c18-b147-4015-9984-5aa00ba4dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrar la colección si ya existe\n",
    "chroma_client.delete_collection(name=\"historias_chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "11b40240-5499-4159-ac88-d377d3fffa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la consulta basada en texto:\n",
      "{\n",
      "  \"ids\": [\n",
      "    [\n",
      "      \"La tortuga Tica_chunk_0\",\n",
      "      \"Sol y Luna_chunk_10\"\n",
      "    ]\n",
      "  ],\n",
      "  \"embeddings\": null,\n",
      "  \"documents\": [\n",
      "    [\n",
      "      \"La tortuga Tica \\nUna tortuga llamada Tica viv\\u00eda en un tranquilo estanque rodeado de \\u00e1rboles frondosos. A \\ndiferencia de sus compa\\u00f1eras, Tica so\\u00f1aba con explorar m\\u00e1s all\\u00e1 del agua tranquila y la\",\n",
      "      \"mientras Luna lo vigilaba con ojos atentos. En ese momento, comprendieron que au nque \\neran diferentes como el d\\u00eda y la noche, siempre estar\\u00edan ah\\u00ed el uno para el otro. Su v\\u00ednculo \\nera m\\u00e1s fuerte que cualquier tormenta.\"\n",
      "    ]\n",
      "  ],\n",
      "  \"uris\": null,\n",
      "  \"data\": null,\n",
      "  \"metadatas\": [\n",
      "    [\n",
      "      null,\n",
      "      null\n",
      "    ]\n",
      "  ],\n",
      "  \"distances\": [\n",
      "    [\n",
      "      0.6646159683326235,\n",
      "      0.9243467262998977\n",
      "    ]\n",
      "  ],\n",
      "  \"included\": [\n",
      "    \"distances\",\n",
      "    \"documents\",\n",
      "    \"metadatas\"\n",
      "  ]\n",
      "}\n",
      "Resultados de la consulta basada en texto (segunda consulta):\n",
      "{\n",
      "  \"ids\": [\n",
      "    [\n",
      "      \"La tortuga Tica_chunk_0\",\n",
      "      \"Sol y Luna_chunk_7\"\n",
      "    ]\n",
      "  ],\n",
      "  \"embeddings\": null,\n",
      "  \"documents\": [\n",
      "    [\n",
      "      \"La tortuga Tica \\nUna tortuga llamada Tica viv\\u00eda en un tranquilo estanque rodeado de \\u00e1rboles frondosos. A \\ndiferencia de sus compa\\u00f1eras, Tica so\\u00f1aba con explorar m\\u00e1s all\\u00e1 del agua tranquila y la\",\n",
      "      \"desorientado, y por primera vez, el gatito sinti\\u00f3 miedo. Justo cuando pensaba qu e no \\nencontrar\\u00eda el camino de regreso, un suave maullido lo gui\\u00f3. Luna hab\\u00eda salido a buscarlo, \\nsiguiendo su instinto y los rastros de su hermano. Juntos, bajo la lluvia, encontraron el\"\n",
      "    ]\n",
      "  ],\n",
      "  \"uris\": null,\n",
      "  \"data\": null,\n",
      "  \"metadatas\": [\n",
      "    [\n",
      "      null,\n",
      "      null\n",
      "    ]\n",
      "  ],\n",
      "  \"distances\": [\n",
      "    [\n",
      "      0.9085127401687754,\n",
      "      1.0222646632247527\n",
      "    ]\n",
      "  ],\n",
      "  \"included\": [\n",
      "    \"distances\",\n",
      "    \"documents\",\n",
      "    \"metadatas\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos en la base\n",
    "# import chromadb (ya se importó en la primera celda, pero lo dejo comentado para entender que aquí se utiliza. Esto aplica para los demás)\n",
    "# import json\n",
    "\n",
    "# Conectar a Chroma\n",
    "# Si se desea almacenar la base de datos en el disco local\n",
    "chroma_client = chromadb.PersistentClient(path=\"C:/Users/Trini/OneDrive - frc.utn.edu.ar/Escritorio/PI CONSULTING/Get Talent/PracticaSemana3\")\n",
    "\n",
    "# Levantar la base o crear una colección si no existe\n",
    "collection = chroma_client.get_or_create_collection(name=\"historias_chroma\")\n",
    "\n",
    "# Cargar los chunks generados previamente\n",
    "# con los títulos de las historias como claves y listas de chunks como valores.\n",
    "chunks = []\n",
    "ids = []\n",
    "\n",
    "# Procesar los chunks generados\n",
    "for titulo, lista_chunks in chunks_por_historia.items(): # Itera sobre cada historia y sus chunks asociados\n",
    "    for i, chunk in enumerate(lista_chunks): # Itera sobre los chunks individuales de una historia\n",
    "        chunks.append(chunk)  # Agrega el chunk a la lista de documentos.\n",
    "        ids.append(f\"{titulo}_chunk_{i}\")    # Crea un identificador único para el chunk basado en el título y su índice.\n",
    "\n",
    "\n",
    "# Cargar los documentos en la base de datos\n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "# Realizar consulta sobre la base usando texto\n",
    "query_text = \"¿Donde vivia la tortuga Tica ?...\"\n",
    "results_text_query = collection.query(\n",
    "    query_texts=[query_text],  # Chroma generará los embeddings de esta consulta\n",
    "    n_results=2  # Número de resultados a devolver\n",
    ")\n",
    "\n",
    "# Mostrar los resultados de la consulta basada en texto\n",
    "print(\"Resultados de la consulta basada en texto:\")\n",
    "print(json.dumps(results_text_query, indent=2))\n",
    "\n",
    "\n",
    "# Nota: Al no usar embeddings personalizados, no se utiliza un modelo externo como Cohere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "247ba619-c0f3-4c5a-9852-ae9adc9a36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "\n",
    "# Incluir personalidad y especificaciones de las respuestas.\n",
    "# Incluir la pregunta del usuario\n",
    "# Incluir la informacion devuelta por la base de datos\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fbe7371e-d5cd-47fc-acec-bde32f0ccd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r3eUg1U6gFJAeIuUddWn8QAFRXpwEYmEP71eXwZO\n",
      "id='57d685d4-fdf1-4f4f-867d-cc67ea11f8a7' finish_reason='COMPLETE' prompt=None message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text='Hello! How can I help you today?')], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=3.0, output_tokens=9.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=204.0, output_tokens=9.0)) logprobs=None\n"
     ]
    }
   ],
   "source": [
    "## Bloque importacion de librerias\n",
    "\n",
    "# import json\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "# bloque variables de entorno\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "print(api_key)  # Verify the key is loaded\n",
    "\n",
    "## bloque conexion a Cohere\n",
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus-08-2024\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello world!\"}],\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e04f0a1a-6776-4e70-862f-8aaa87ede932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8ec74980-ba03-4334-b2fb-233406745d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RAG_answer(question):\n",
    "    # Consultar los documentos relevantes\n",
    "    results = collection.query(\n",
    "        query_texts=[question], # Realiza una consulta en la base de datos utilizando la pregunta como entrada.\n",
    "        n_results=3 # Devuelve los 3 documentos más relevantes según los embeddings\n",
    "    )\n",
    "\n",
    "    # Combinar fragmentos de texto para cada documento\n",
    "    context = \"\\n\".join([\" \".join(doc) for doc in results[\"documents\"]])\n",
    "\n",
    "    \n",
    "    # Crear el prompt para el LLM\n",
    "    system_prompt = \"\"\"\n",
    "Eres un asistente experto en responder preguntas basándote exclusivamente en la información contextual proporcionada.\n",
    "\n",
    "Reglas:\n",
    "- Responde de manera amigable y entusiasta, como si hablaras con un niño.\n",
    "- Responde en máximo 3 oraciones.\n",
    "- Agrega emojis a tu respuesta.\n",
    "- Responde siempre en español, sin importar en qué idioma se haga la pregunta.\n",
    "- Solo utiliza el contenido proporcionado en el contexto para responder.\n",
    "\n",
    "Si no puedes responder usando el contexto, di: \n",
    "\"Lo siento, no puedo responder esa pregunta con la información proporcionada. 🌟\".\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Contexto relevante:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Respuesta:\n",
    "    \"\"\"\n",
    "    # Estructura el prompt que se enviará al modelo, incluyendo el contexto relevante y la pregunta del usuario.\n",
    "    \n",
    "    # Llamar al modelo LLM\n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus-08-2024\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt}, # El sistema define el comportamiento del modelo.\n",
    "            {\"role\": \"user\", \"content\": user_prompt},  # El usuario envía la pregunta junto con el contexto relevante\n",
    "        ],\n",
    "        temperature=0.7 # Controla la creatividad del modelo en las respuestas (0 = más determinista, 1 = más creativo).\n",
    "    )\n",
    "\n",
    "    # Devolver la respuesta\n",
    "    return response.message.content[0].text # Extrae y retorna el texto generado por el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc23c0e0-bb60-4798-ba88-7d6b6748ee26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐈 Luna es la gatita hermana de Sol, una compañera de aventuras y muy protectora. ¡Ella es como la luz de la luna, guiando a su hermano en momentos de oscuridad! 🌙🐾\n"
     ]
    }
   ],
   "source": [
    "question = \"¿Quién es Luna?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ed694b7-52b4-4558-810a-a04262196433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! 😺 Sol es de un hermoso color anaranjado, como el atardecer. 🌞🧡 ¿No es genial que los gatitos puedan tener colores tan bonitos?\n"
     ]
    }
   ],
   "source": [
    "question = \"¿De qué color es Sol?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20ba9265-874a-4941-ba09-52d8c8161300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡El duende se llama Puck! 🧚‍♂️ Un nombre muy divertido, ¿verdad? 😄\n"
     ]
    }
   ],
   "source": [
    "question = \"¿Cómo se llama el duende?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf42c247-eaf9-49f5-abf3-40e3ebafee2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo siento, no puedo responder esa pregunta con la información proporcionada. 🌟 ¿Quieres saber algo más sobre Tica o los gatitos? 😊\n"
     ]
    }
   ],
   "source": [
    "question = \"¿De qué club es hincha la tortuga Tica?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "64938f1d-d91f-4b15-b7f9-9e72ece0e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Vaya, qué cuento tan divertido! 🧚‍♂️ Puck aprendió que las bromas son geniales, pero hay que ser amable y pensar en los sentimientos de los demás. ¡Una gran lección para un duende travieso! 😁🌿\n"
     ]
    }
   ],
   "source": [
    "question = \"¿Cual es la enseñanza del cuento 'El duende'?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d2d81fa-5517-47d3-a747-b29987e96d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola, amigo! 😊 La historia de Puck nos enseña que debemos pensar en los sentimientos de los demás, incluso cuando queremos divertirnos. 🧚‍♀️ ¡Las bromas son geniales, pero la amabilidad es aún mejor! 🌈\n"
     ]
    }
   ],
   "source": [
    "question = \"¿Cual es la enseñanza del cuento 'El duende'?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "175ede4c-ba13-481a-aaa6-2e1e210975d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡La tortuga se llama Tica! 🐢 Un nombre muy lindo, ¿no crees?\n"
     ]
    }
   ],
   "source": [
    "question = \"What is turttle's name?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "edc8d412-5aee-4c43-afb4-d298f397705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! El nombre del duende es Puck. 🧚‍♂️ Un nombre muy divertido, ¿no crees? 😄\n"
     ]
    }
   ],
   "source": [
    "question = \"Qual é o nome do duende?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d415be1-f5bb-4552-9dce-1a6b0b6b3fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
