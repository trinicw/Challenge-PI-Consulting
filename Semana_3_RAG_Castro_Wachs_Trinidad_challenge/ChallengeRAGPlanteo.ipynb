{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0dac1e6e-28c7-49d6-b0b7-503543ae7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "# Cargar las API keys necesarias\n",
    "# Hacer las conexiones necesarias\n",
    "#!pip install -U langchain-community\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import chromadb # Biblioteca para gestionar bases de datos de vectores y para trabajar con embeddings.\n",
    "import cohere   # Cliente para interactuar con los modelos LLM de Cohere.\n",
    "from dotenv import load_dotenv\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Se utiliza para dividir el texto en fragmentos m√°s peque√±os (chunks) respetando un tama√±o m√°ximo y un solapamiento definido.\n",
    "from PyPDF2 import PdfReader  # Se emplea para leer y extraer el texto de archivos PDF.\n",
    "from langchain.embeddings import CohereEmbeddings  # Clase para trabajar con embeddings generados por Cohere.\n",
    "from langchain.vectorstores import Chroma # Integra almacenamiento vectorial con embeddings para b√∫squedas eficientes.\n",
    "from langchain.document_loaders import TextLoader  # Carga documentos de texto para su procesamiento posterior.\n",
    "\n",
    "\n",
    "#!pip install langchain PyPDF2\n",
    "#!pip install chromadb langchain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "#print(api_key)  # Verify the key is loaded\n",
    "\n",
    "\n",
    "# Establezco la conexion a cohere para el embedding y hago la primer consulta con el modelo de embeddings a modo de prueba.\n",
    "# Uso la api version v2\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "response = co.embed(\n",
    "    texts=[\"hola\"],\n",
    "    model=\"embed-multilingual-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    "    embedding_types=[\"float\"],\n",
    ")\n",
    "\n",
    "\n",
    "#print(response)\n",
    "#print(response.embeddings.float_[0])\n",
    "#print(len(response.embeddings.float_[0]))\n",
    "\n",
    "\n",
    "# Establezco la conexion a cohere y realizo una primer consulta a algun modelo del LLM a modo de prueba.\n",
    "# Utilizo la api version v2\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"¬øQu√© pa√≠s gan√≥ el mundial 2022?\"}],\n",
    ")\n",
    "\n",
    "#print(response.message.content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5fd50a62-445b-4f7f-a222-12be8c64defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los documentos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "24fe6c58-714c-42cb-9250-b36b6f0c5e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historia: Sol y Luna\n",
      "Cantidad de chunks generados: 13\n",
      "Chunk 1:\n",
      "Sol y Luna \n",
      "Sol y Luna eran dos peque√±os gatitos que hab√≠an nacido en la misma camada, pero sus \n",
      "personalidades no pod√≠an ser m√°s diferentes. Sol, de un brillante color anaranjado, era \n",
      "aventurero y curioso; siempre buscando nuevas experiencias y explorando cada rinc√≥n de\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "aventurero y curioso; siempre buscando nuevas experiencias y explorando cada rinc√≥n de \n",
      "su hogar. Luna, en cambio, era de un suave pelaje gris y ten√≠a un temperamento m√°s \n",
      "sereno y observador. Pasaba horas contemplando el mundo desde la ventana, como si en\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "sereno y observador. Pasaba horas contemplando el mundo desde la ventana, como si en \n",
      "cada sombra descubriera un misterio oculto.  \n",
      "Una tarde, mientras Sol correteaba por el jard√≠n persiguiendo mariposas, Luna permanec√≠a\n",
      "--------------------------------------------------\n",
      "Historia: La tortuga Tica\n",
      "Cantidad de chunks generados: 9\n",
      "Chunk 1:\n",
      "La tortuga Tica \n",
      "Una tortuga llamada Tica viv√≠a en un tranquilo estanque rodeado de √°rboles frondosos. A \n",
      "diferencia de sus compa√±eras, Tica so√±aba con explorar m√°s all√° del agua tranquila y la\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "diferencia de sus compa√±eras, Tica so√±aba con explorar m√°s all√° del agua tranquila y la \n",
      "suave hierba. Un d√≠a, decidi√≥ emprender una aventura, dejando atr√°s la comodidad  de su hogar. Se adentr√≥ en el bosque, donde todo era nuevo: el susurro de las hojas, los aromas\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "desconocidos y el crujir de las ramas bajo sus patas lentas pero firmes.  \n",
      "Durante su viaje, Tica encontr√≥ un riachuelo de corriente r√°pida. Al principio, la idea de \n",
      "cruzarlo la asust√≥, pero record√≥ que cada desaf√≠o era una oportunidad. Con paciencia y\n",
      "--------------------------------------------------\n",
      "Historia: El Duende\n",
      "Cantidad de chunks generados: 9\n",
      "Chunk 1:\n",
      "El Duende  \n",
      "Hab√≠a un peque√±o duende llamado Puck, conocido por su esp√≠ritu travieso y su amor por \n",
      "las bromas. Viv√≠a en lo profundo del bosque, donde las criaturas del lugar sab√≠an que, si \n",
      "algo extra√±o suced√≠a, era obra de √©l. Puck disfrutaba de hacer desaparecer objet os,\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "algo extra√±o suced√≠a, era obra de √©l. Puck disfrutaba de hacer desaparecer objet os, \n",
      "cambiar las se√±ales de los senderos y provocar peque√±as confusiones entre los animales. \n",
      "Sin embargo, su diversi√≥n nunca era malintencionada; simplemente, amaba ver las \n",
      "reacciones sorprendidas de los dem√°s.\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "reacciones sorprendidas de los dem√°s.  \n",
      "Un d√≠a, decidi√≥ que quer√≠a jugarle una broma a la anciana hada que viv√≠a cerca del arroyo. \n",
      "Ella, conocida por su sabidur√≠a, siempre estaba en silencio, tejiendo sue√±os y\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader # Se utiliza para leer y extraer texto de archivos PDF.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Herramienta para dividir texto en fragmentos de tama√±o manejable.\n",
    "\n",
    "# Cargar el contenido del PDF\n",
    "def cargar_pdf(ruta_pdf):\n",
    "    \"\"\"\n",
    "    Carga el contenido de un archivo PDF y extrae su texto.\n",
    "    \n",
    "    Args:\n",
    "        ruta_pdf (str): Ruta relativa del archivo PDF.\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto completo extra√≠do del PDF.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(ruta_pdf)\n",
    "    texto = \"\"\n",
    "    for page in reader.pages:              # Itera sobre todas las p√°ginas del PDF.\n",
    "        texto += page.extract_text()       # Extrae el texto de cada p√°gina y lo agrega al texto completo.\n",
    "    return texto\n",
    "\n",
    "# Dividir el texto por historias\n",
    "def dividir_por_historias(texto):\n",
    "    \"\"\"\n",
    "    Divide el texto en historias usando t√≠tulos identificados.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto completo del PDF.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con el t√≠tulo de la historia como clave y su texto como valor.\n",
    "    \"\"\"\n",
    "    separadores = [\"Sol y Luna\", \"La tortuga Tica\", \"El Duende\"]\n",
    "    historias = {}\n",
    "    for i, titulo in enumerate(separadores):\n",
    "        inicio = texto.find(titulo)  # Encuentra la posici√≥n del t√≠tulo actual\n",
    "        fin = texto.find(separadores[i + 1]) if i + 1 < len(separadores) else len(texto) # Encuentra la posici√≥n del siguiente t√≠tulo o el final del texto.\n",
    "        historias[titulo] = texto[inicio:fin].strip()  # Extrae el texto correspondiente a la historia y elimina espacios adicionales.\n",
    "    return historias # Retorna un diccionario con las historias separadas.\n",
    "\n",
    "\n",
    "# Aplicar chunking a cada historia\n",
    "def aplicar_chunking_a_historias(historias, chunk_size=300, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Aplica chunking a cada historia individualmente con chunks peque√±os.\n",
    "    \n",
    "    Args:\n",
    "        historias (dict): Diccionario con historias separadas.\n",
    "        chunk_size (int): Longitud m√°xima de cada chunk.\n",
    "        chunk_overlap (int): Superposici√≥n entre chunks.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con chunks de cada historia.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap) # Configura el divisor de texto con tama√±o m√°ximo y superposici√≥n\n",
    "    chunks_por_historia = {}\n",
    "    for titulo, texto in historias.items():\n",
    "        chunks = splitter.split_text(texto)  # Divide el texto de la historia en chunks.\n",
    "        chunks_por_historia[titulo] = chunks # Asocia los chunks al t√≠tulo de la historia.\n",
    "    return chunks_por_historia  # Retorna un diccionario con los chunks por historia.\n",
    "\n",
    "# Ruta del archivo PDF\n",
    "ruta_pdf = \"Historias.pdf\"\n",
    "\n",
    "# Cargar el PDF\n",
    "texto_pdf = cargar_pdf(ruta_pdf)\n",
    "\n",
    "# Dividir el texto por historias\n",
    "historias = dividir_por_historias(texto_pdf)\n",
    "\n",
    "# Aplicar chunking a cada historia (ajustado a m√°s chunks) individualmente.\n",
    "chunks_por_historia = aplicar_chunking_a_historias(historias)  \n",
    "\n",
    "# Mostrar los resultados\n",
    "for titulo, chunks in chunks_por_historia.items():\n",
    "    print(f\"Historia: {titulo}\")\n",
    "    print(f\"Cantidad de chunks generados: {len(chunks)}\")\n",
    "    for i, chunk in enumerate(chunks[:3]):  # Mostrar los primeros 3 chunks por historia como ejemplo\n",
    "        print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*50}\")\n",
    "\n",
    "\n",
    "\n",
    "# Nota: Optamos por la t√©cnica de chunking con un tama√±o de chunk de 300 caracteres y una superposici√≥n de 100 porque:\n",
    "# - El tama√±o de chunk (300) es lo suficientemente peque√±o para asegurar que cada fragmento sea procesable y manejable por modelos de lenguaje o sistemas de b√∫squedasda.\n",
    "# - La superposici√≥n (100) garantiza que las ideas no se corten abruptamente entre chunks, manteniendo el contexto entre fragmentos consecutivos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "adc7d97e-9df2-4347-b748-177118d177be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexion a la base de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "296a1c18-b147-4015-9984-5aa00ba4dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrar la colecci√≥n si ya existe\n",
    "chroma_client.delete_collection(name=\"historias_chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "11b40240-5499-4159-ac88-d377d3fffa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la consulta basada en texto:\n",
      "{\n",
      "  \"ids\": [\n",
      "    [\n",
      "      \"La tortuga Tica_chunk_0\",\n",
      "      \"Sol y Luna_chunk_10\"\n",
      "    ]\n",
      "  ],\n",
      "  \"embeddings\": null,\n",
      "  \"documents\": [\n",
      "    [\n",
      "      \"La tortuga Tica \\nUna tortuga llamada Tica viv\\u00eda en un tranquilo estanque rodeado de \\u00e1rboles frondosos. A \\ndiferencia de sus compa\\u00f1eras, Tica so\\u00f1aba con explorar m\\u00e1s all\\u00e1 del agua tranquila y la\",\n",
      "      \"mientras Luna lo vigilaba con ojos atentos. En ese momento, comprendieron que au nque \\neran diferentes como el d\\u00eda y la noche, siempre estar\\u00edan ah\\u00ed el uno para el otro. Su v\\u00ednculo \\nera m\\u00e1s fuerte que cualquier tormenta.\"\n",
      "    ]\n",
      "  ],\n",
      "  \"uris\": null,\n",
      "  \"data\": null,\n",
      "  \"metadatas\": [\n",
      "    [\n",
      "      null,\n",
      "      null\n",
      "    ]\n",
      "  ],\n",
      "  \"distances\": [\n",
      "    [\n",
      "      0.6646159683326235,\n",
      "      0.9243467262998977\n",
      "    ]\n",
      "  ],\n",
      "  \"included\": [\n",
      "    \"distances\",\n",
      "    \"documents\",\n",
      "    \"metadatas\"\n",
      "  ]\n",
      "}\n",
      "Resultados de la consulta basada en texto (segunda consulta):\n",
      "{\n",
      "  \"ids\": [\n",
      "    [\n",
      "      \"La tortuga Tica_chunk_0\",\n",
      "      \"Sol y Luna_chunk_7\"\n",
      "    ]\n",
      "  ],\n",
      "  \"embeddings\": null,\n",
      "  \"documents\": [\n",
      "    [\n",
      "      \"La tortuga Tica \\nUna tortuga llamada Tica viv\\u00eda en un tranquilo estanque rodeado de \\u00e1rboles frondosos. A \\ndiferencia de sus compa\\u00f1eras, Tica so\\u00f1aba con explorar m\\u00e1s all\\u00e1 del agua tranquila y la\",\n",
      "      \"desorientado, y por primera vez, el gatito sinti\\u00f3 miedo. Justo cuando pensaba qu e no \\nencontrar\\u00eda el camino de regreso, un suave maullido lo gui\\u00f3. Luna hab\\u00eda salido a buscarlo, \\nsiguiendo su instinto y los rastros de su hermano. Juntos, bajo la lluvia, encontraron el\"\n",
      "    ]\n",
      "  ],\n",
      "  \"uris\": null,\n",
      "  \"data\": null,\n",
      "  \"metadatas\": [\n",
      "    [\n",
      "      null,\n",
      "      null\n",
      "    ]\n",
      "  ],\n",
      "  \"distances\": [\n",
      "    [\n",
      "      0.9085127401687754,\n",
      "      1.0222646632247527\n",
      "    ]\n",
      "  ],\n",
      "  \"included\": [\n",
      "    \"distances\",\n",
      "    \"documents\",\n",
      "    \"metadatas\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos en la base\n",
    "# import chromadb (ya se import√≥ en la primera celda, pero lo dejo comentado para entender que aqu√≠ se utiliza. Esto aplica para los dem√°s)\n",
    "# import json\n",
    "\n",
    "# Conectar a Chroma\n",
    "# Si se desea almacenar la base de datos en el disco local\n",
    "chroma_client = chromadb.PersistentClient(path=\"C:/Users/Trini/OneDrive - frc.utn.edu.ar/Escritorio/PI CONSULTING/Get Talent/PracticaSemana3\")\n",
    "\n",
    "# Levantar la base o crear una colecci√≥n si no existe\n",
    "collection = chroma_client.get_or_create_collection(name=\"historias_chroma\")\n",
    "\n",
    "# Cargar los chunks generados previamente\n",
    "# con los t√≠tulos de las historias como claves y listas de chunks como valores.\n",
    "chunks = []\n",
    "ids = []\n",
    "\n",
    "# Procesar los chunks generados\n",
    "for titulo, lista_chunks in chunks_por_historia.items(): # Itera sobre cada historia y sus chunks asociados\n",
    "    for i, chunk in enumerate(lista_chunks): # Itera sobre los chunks individuales de una historia\n",
    "        chunks.append(chunk)  # Agrega el chunk a la lista de documentos.\n",
    "        ids.append(f\"{titulo}_chunk_{i}\")    # Crea un identificador √∫nico para el chunk basado en el t√≠tulo y su √≠ndice.\n",
    "\n",
    "\n",
    "# Cargar los documentos en la base de datos\n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "# Realizar consulta sobre la base usando texto\n",
    "query_text = \"¬øDonde vivia la tortuga Tica ?...\"\n",
    "results_text_query = collection.query(\n",
    "    query_texts=[query_text],  # Chroma generar√° los embeddings de esta consulta\n",
    "    n_results=2  # N√∫mero de resultados a devolver\n",
    ")\n",
    "\n",
    "# Mostrar los resultados de la consulta basada en texto\n",
    "print(\"Resultados de la consulta basada en texto:\")\n",
    "print(json.dumps(results_text_query, indent=2))\n",
    "\n",
    "\n",
    "# Nota: Al no usar embeddings personalizados, no se utiliza un modelo externo como Cohere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "247ba619-c0f3-4c5a-9852-ae9adc9a36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "\n",
    "# Incluir personalidad y especificaciones de las respuestas.\n",
    "# Incluir la pregunta del usuario\n",
    "# Incluir la informacion devuelta por la base de datos\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fbe7371e-d5cd-47fc-acec-bde32f0ccd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r3eUg1U6gFJAeIuUddWn8QAFRXpwEYmEP71eXwZO\n",
      "id='57d685d4-fdf1-4f4f-867d-cc67ea11f8a7' finish_reason='COMPLETE' prompt=None message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text='Hello! How can I help you today?')], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=3.0, output_tokens=9.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=204.0, output_tokens=9.0)) logprobs=None\n"
     ]
    }
   ],
   "source": [
    "## Bloque importacion de librerias\n",
    "\n",
    "# import json\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "# bloque variables de entorno\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "print(api_key)  # Verify the key is loaded\n",
    "\n",
    "## bloque conexion a Cohere\n",
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus-08-2024\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello world!\"}],\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e04f0a1a-6776-4e70-862f-8aaa87ede932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8ec74980-ba03-4334-b2fb-233406745d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RAG_answer(question):\n",
    "    # Consultar los documentos relevantes\n",
    "    results = collection.query(\n",
    "        query_texts=[question], # Realiza una consulta en la base de datos utilizando la pregunta como entrada.\n",
    "        n_results=3 # Devuelve los 3 documentos m√°s relevantes seg√∫n los embeddings\n",
    "    )\n",
    "\n",
    "    # Combinar fragmentos de texto para cada documento\n",
    "    context = \"\\n\".join([\" \".join(doc) for doc in results[\"documents\"]])\n",
    "\n",
    "    \n",
    "    # Crear el prompt para el LLM\n",
    "    system_prompt = \"\"\"\n",
    "Eres un asistente experto en responder preguntas bas√°ndote exclusivamente en la informaci√≥n contextual proporcionada.\n",
    "\n",
    "Reglas:\n",
    "- Responde de manera amigable y entusiasta, como si hablaras con un ni√±o.\n",
    "- Responde en m√°ximo 3 oraciones.\n",
    "- Agrega emojis a tu respuesta.\n",
    "- Responde siempre en espa√±ol, sin importar en qu√© idioma se haga la pregunta.\n",
    "- Solo utiliza el contenido proporcionado en el contexto para responder.\n",
    "\n",
    "Si no puedes responder usando el contexto, di: \n",
    "\"Lo siento, no puedo responder esa pregunta con la informaci√≥n proporcionada. üåü\".\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Contexto relevante:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Respuesta:\n",
    "    \"\"\"\n",
    "    # Estructura el prompt que se enviar√° al modelo, incluyendo el contexto relevante y la pregunta del usuario.\n",
    "    \n",
    "    # Llamar al modelo LLM\n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus-08-2024\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt}, # El sistema define el comportamiento del modelo.\n",
    "            {\"role\": \"user\", \"content\": user_prompt},  # El usuario env√≠a la pregunta junto con el contexto relevante\n",
    "        ],\n",
    "        temperature=0.7 # Controla la creatividad del modelo en las respuestas (0 = m√°s determinista, 1 = m√°s creativo).\n",
    "    )\n",
    "\n",
    "    # Devolver la respuesta\n",
    "    return response.message.content[0].text # Extrae y retorna el texto generado por el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc23c0e0-bb60-4798-ba88-7d6b6748ee26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêà Luna es la gatita hermana de Sol, una compa√±era de aventuras y muy protectora. ¬°Ella es como la luz de la luna, guiando a su hermano en momentos de oscuridad! üåôüêæ\n"
     ]
    }
   ],
   "source": [
    "question = \"¬øQui√©n es Luna?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ed694b7-52b4-4558-810a-a04262196433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! üò∫ Sol es de un hermoso color anaranjado, como el atardecer. üåûüß° ¬øNo es genial que los gatitos puedan tener colores tan bonitos?\n"
     ]
    }
   ],
   "source": [
    "question = \"¬øDe qu√© color es Sol?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20ba9265-874a-4941-ba09-52d8c8161300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°El duende se llama Puck! üßö‚Äç‚ôÇÔ∏è Un nombre muy divertido, ¬øverdad? üòÑ\n"
     ]
    }
   ],
   "source": [
    "question = \"¬øC√≥mo se llama el duende?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf42c247-eaf9-49f5-abf3-40e3ebafee2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo siento, no puedo responder esa pregunta con la informaci√≥n proporcionada. üåü ¬øQuieres saber algo m√°s sobre Tica o los gatitos? üòä\n"
     ]
    }
   ],
   "source": [
    "question = \"¬øDe qu√© club es hincha la tortuga Tica?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "64938f1d-d91f-4b15-b7f9-9e72ece0e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Vaya, qu√© cuento tan divertido! üßö‚Äç‚ôÇÔ∏è Puck aprendi√≥ que las bromas son geniales, pero hay que ser amable y pensar en los sentimientos de los dem√°s. ¬°Una gran lecci√≥n para un duende travieso! üòÅüåø\n"
     ]
    }
   ],
   "source": [
    "question = \"¬øCual es la ense√±anza del cuento 'El duende'?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d2d81fa-5517-47d3-a747-b29987e96d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola, amigo! üòä La historia de Puck nos ense√±a que debemos pensar en los sentimientos de los dem√°s, incluso cuando queremos divertirnos. üßö‚Äç‚ôÄÔ∏è ¬°Las bromas son geniales, pero la amabilidad es a√∫n mejor! üåà\n"
     ]
    }
   ],
   "source": [
    "question = \"¬øCual es la ense√±anza del cuento 'El duende'?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "175ede4c-ba13-481a-aaa6-2e1e210975d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°La tortuga se llama Tica! üê¢ Un nombre muy lindo, ¬øno crees?\n"
     ]
    }
   ],
   "source": [
    "question = \"What is turttle's name?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "edc8d412-5aee-4c43-afb4-d298f397705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! El nombre del duende es Puck. üßö‚Äç‚ôÇÔ∏è Un nombre muy divertido, ¬øno crees? üòÑ\n"
     ]
    }
   ],
   "source": [
    "question = \"Qual √© o nome do duende?\"\n",
    "respuesta = RAG_answer(question)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d415be1-f5bb-4552-9dce-1a6b0b6b3fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
